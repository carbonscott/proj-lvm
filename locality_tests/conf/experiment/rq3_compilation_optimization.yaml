# @package _global_
# RQ3: Compilation Optimization Effectiveness
#
# Research Question: Under what conditions does torch compile provide significant 
# performance improvements for ViT inference, and how does effectiveness vary with model characteristics?
#
# Sub-Questions:
# - RQ3.1: Do smaller models benefit more from compilation (kernel launch overhead) vs larger models (kernel fusion)?
# - RQ3.2: How does compilation effectiveness vary with batch size and input resolution?
# - RQ3.3: Which ViT operations see the most significant compilation speedup?
# - RQ3.4: What is the compilation overhead cost vs performance benefit?

experiment:
  name: "rq3_compilation_optimization"
  description: "Study torch.compile optimization effects across different model complexities and compilation modes (default, reduce-overhead, max-autotune)"
  max_workers: 1  # Sequential execution for accurate measurements
  resume: true

  test:
    num_samples: 2000
    batch_size: 8
    warmup_samples: 300  # Increased warmup for compilation studies
    memory_size_mb: 1024
    tensor_shape: [3, 224, 224]
    compile_model: false  # Will be overridden per config
    timeout_s: 1200  # Increased timeout for compilation overhead

  hardware:
    gpu_ids: [0]  # Single GPU for clean comparison
    numa_nodes: [0]  # Use local NUMA node for best performance baseline

  # Systematic comparison of compiled vs uncompiled across model complexities
  # Each model tested in both compiled and uncompiled variants
  vit_configs:
    # === UNCOMPILED BASELINE MODELS ===
    
    # Small models - should show kernel launch overhead benefits
    - name: "vit32x6x384_uncompiled"
      patch_size: 32
      depth: 6
      heads: 6
      dim: 384
      mlp_dim: 1536
      compile_model: false
    
    - name: "vit16x6x384_uncompiled"
      patch_size: 16
      depth: 6
      heads: 6
      dim: 384
      mlp_dim: 1536
      compile_model: false
    
    - name: "vit32x6x768_uncompiled"
      patch_size: 32
      depth: 6
      heads: 12
      dim: 768
      mlp_dim: 3072
      compile_model: false

    # Medium models - balanced kernel launch vs fusion benefits
    - name: "vit32x12x384_uncompiled"
      patch_size: 32
      depth: 12
      heads: 6
      dim: 384
      mlp_dim: 1536
      compile_model: false
    
    - name: "vit16x12x768_uncompiled"
      patch_size: 16
      depth: 12
      heads: 12
      dim: 768
      mlp_dim: 3072
      compile_model: false
    
    - name: "vit32x12x768_uncompiled"
      patch_size: 32
      depth: 12
      heads: 12
      dim: 768
      mlp_dim: 3072
      compile_model: false
    
    - name: "vit32x12x1024_uncompiled"
      patch_size: 32
      depth: 12
      heads: 16
      dim: 1024
      mlp_dim: 4096
      compile_model: false

    # Large models - should show kernel fusion benefits
    - name: "vit32x18x768_uncompiled"
      patch_size: 32
      depth: 18
      heads: 12
      dim: 768
      mlp_dim: 3072
      compile_model: false
    
    - name: "vit32x18x1024_uncompiled"
      patch_size: 32
      depth: 18
      heads: 16
      dim: 1024
      mlp_dim: 4096
      compile_model: false
    
    - name: "vit16x24x768_uncompiled"
      patch_size: 16
      depth: 24
      heads: 12
      dim: 768
      mlp_dim: 3072
      compile_model: false
    
    - name: "vit32x24x1024_uncompiled"
      patch_size: 32
      depth: 24
      heads: 16
      dim: 1024
      mlp_dim: 4096
      compile_model: false

    # === COMPILED OPTIMIZED MODELS ===
    
    # Small models - compiled versions
    - name: "vit32x6x384_compiled"
      patch_size: 32
      depth: 6
      heads: 6
      dim: 384
      mlp_dim: 1536
      compile_model: true
    
    - name: "vit16x6x384_compiled"
      patch_size: 16
      depth: 6
      heads: 6
      dim: 384
      mlp_dim: 1536
      compile_model: true
    
    - name: "vit32x6x768_compiled"
      patch_size: 32
      depth: 6
      heads: 12
      dim: 768
      mlp_dim: 3072
      compile_model: true

    # Medium models - compiled versions
    - name: "vit32x12x384_compiled"
      patch_size: 32
      depth: 12
      heads: 6
      dim: 384
      mlp_dim: 1536
      compile_model: true
    
    - name: "vit16x12x768_compiled"
      patch_size: 16
      depth: 12
      heads: 12
      dim: 768
      mlp_dim: 3072
      compile_model: true
    
    - name: "vit32x12x768_compiled"
      patch_size: 32
      depth: 12
      heads: 12
      dim: 768
      mlp_dim: 3072
      compile_model: true
    
    - name: "vit32x12x1024_compiled"
      patch_size: 32
      depth: 12
      heads: 16
      dim: 1024
      mlp_dim: 4096
      compile_model: true

    # Large models - compiled versions
    - name: "vit32x18x768_compiled"
      patch_size: 32
      depth: 18
      heads: 12
      dim: 768
      mlp_dim: 3072
      compile_model: true
    
    - name: "vit32x18x1024_compiled"
      patch_size: 32
      depth: 18
      heads: 16
      dim: 1024
      mlp_dim: 4096
      compile_model: true
    
    - name: "vit16x24x768_compiled"
      patch_size: 16
      depth: 24
      heads: 12
      dim: 768
      mlp_dim: 3072
      compile_model: true
    
    - name: "vit32x24x1024_compiled"
      patch_size: 32
      depth: 24
      heads: 16
      dim: 1024
      mlp_dim: 4096
      compile_model: true

    # === BATCH SIZE SENSITIVITY TESTS ===
    # Representative models for batch size vs compilation interaction (RQ3.2)
    
    # Small batch (batch=1) - test compilation overhead vs benefit
    - name: "vit32x12x768_uncompiled_batch1"
      patch_size: 32
      depth: 12
      heads: 12
      dim: 768
      mlp_dim: 3072
      compile_model: false
      batch_size: 1
    
    - name: "vit32x12x768_compiled_batch1"
      patch_size: 32
      depth: 12
      heads: 12
      dim: 768
      mlp_dim: 3072
      compile_model: true
      batch_size: 1

    # Large batch (batch=32) - test compilation effectiveness at high throughput
    - name: "vit32x12x768_uncompiled_batch32"
      patch_size: 32
      depth: 12
      heads: 12
      dim: 768
      mlp_dim: 3072
      compile_model: false
      batch_size: 32
    
    - name: "vit32x12x768_compiled_batch32"
      patch_size: 32
      depth: 12
      heads: 12
      dim: 768
      mlp_dim: 3072
      compile_model: true
      batch_size: 32

    # === ADVANCED COMPILATION MODES ===
    # Test reduce-overhead and max-autotune modes for kernel optimization
    # Focused on models showing kernel launch overhead (small kernels, low GPU utilization)
    
    # Small model - expect biggest impact from kernel optimization
    - name: "vit16x6x384_compiled_reduce-overhead"
      patch_size: 16
      depth: 6
      heads: 6
      dim: 384
      mlp_dim: 1536
      compile_model: true
      compile_mode: "reduce-overhead"
    
    - name: "vit16x6x384_compiled_max-autotune"
      patch_size: 16
      depth: 6
      heads: 6
      dim: 384
      mlp_dim: 1536
      compile_model: true
      compile_mode: "max-autotune"

    # Medium model - performance threshold model (768-dim boundary)
    - name: "vit32x12x768_compiled_reduce-overhead"
      patch_size: 32
      depth: 12
      heads: 12
      dim: 768
      mlp_dim: 3072
      compile_model: true
      compile_mode: "reduce-overhead"
    
    - name: "vit32x12x768_compiled_max-autotune"
      patch_size: 32
      depth: 12
      heads: 12
      dim: 768
      mlp_dim: 3072
      compile_model: true
      compile_mode: "max-autotune"

    # Large model - compute-bound comparison
    - name: "vit32x18x1024_compiled_reduce-overhead"
      patch_size: 32
      depth: 18
      heads: 16
      dim: 1024
      mlp_dim: 4096
      compile_model: true
      compile_mode: "reduce-overhead"
    
    - name: "vit32x18x1024_compiled_max-autotune"
      patch_size: 32
      depth: 18
      heads: 16
      dim: 1024
      mlp_dim: 4096
      compile_model: true
      compile_mode: "max-autotune"

# Multi-run sweep examples:
# Batch size scaling: python hydra_experiment_runner.py -m experiment=rq3_compilation_optimization experiment.test.batch_size=1,2,4,8,16,32
# Resolution scaling: python hydra_experiment_runner.py -m experiment=rq3_compilation_optimization experiment.test.tensor_shape="[3,224,224],[3,512,512]"
# Advanced compilation modes: python hydra_experiment_runner.py experiment=rq3_compilation_optimization hydra.job.name=advanced_compile_modes